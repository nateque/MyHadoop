#####################################################
		Hadoop 3 Single Node
#####################################################



# Install Java
sudo apt-get update
sudo apt install default-jdk -y    #default version of jdk
java -version


#Create a Hadoop user for accessing HDFS and MapReduce
sudo addgroup hadoop
sudo adduser hduser --ingroup hadoop    #creating hduser in group Hadoop
sudo adduser hduser sudo    #adding hduser in sudo group
sudo su hduser 


#Configure SSH
ssh-keygen    #generating key
cd .ssh   
cat id_rsa.pub >> authorized_keys
ssh localhost


#Download Hadoop tarball(package)
wget -c https://downloads.apache.org/hadoop/common/stable/hadoop-3.3.1.tar.gz


#Extract and Install Hadoop tar ball
tar -xzvf hadoop-3.3.1.tar.gz      #extracting the tarball
sudo mv hadoop-3.3.1 /usr/local/hadoop       #install the package by moving in /usr/local/hadoop
sudo chown hduser:hadoop -R /usr/local/hadoop    

# Set Enviornment Variable
echo $PATH    #to check enviornment variables
readlink -f $(which java)
nano ~/.bashrc

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export PATH=$PATH:/usr/local/hadoop/bin/
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop

source ~/.bashrc

cd /usr/local/hadoop/etc/hadoop/

#Update hadoop-env.sh
nano hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_LOG_DIR=/var/log/hadoop
sudo mkdir /var/log/hadoop
sudo chown hduser:hadoop -R /var/log/hadoop

#Update core-site.xml
nano core-site.xml
 <property>
  <name>fs.defaultFS</name>     
  <value>hdfs://localhost:54310</value>     #where the name node running
</property>


#Update hdfs-site.xml
nano hdfs-site.xml

<property> 
<name>dfs.replication</name>     #defining data copies in HDFS to only 1 instead of 3
  <value>1</value>
 </property>
 <property>
   <name>dfs.namenode.name.dir</name>       #where to put data released by namenode
   <value>file:/usr/local/hadoop_store/hdfs/namenode</value>
 </property>
 <property>
   <name>dfs.datanode.data.dir</name>       #where to put data released by datanode
   <value>file:/usr/local/hadoop_store/hdfs/datanode</value>
 </property>

sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode
sudo chown -R hduser:hadoop /usr/local/hadoop_store


#Update yarn-site.xml
nano yarn-site.xml
<property>
      <name>yarn.nodemanager.aux-services</name>    #we are telling yarn to run MapReduce
      <value>mapreduce_shuffle</value>
   </property>
<property>
        <name>yarn.nodemanager.env-whitelist</name>    #variables required for YARN Processing.
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
</property>


#Update mapred-site.xml
nano mapred-site.xml
<property>
  <name>mapreduce.jobtracker.address</name>    #job tracking from master resource manager
  <value>localhost:54311</value>
   </property>
<property>
   <name>mapreduce.framework.name</name>     #MapReduce run by YARN
   <value>yarn</value>
 </property>
<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME</value>
</property>
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME</value>
</property>
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME</value>
</property>


#Format Namenode
cd 
hdfs namenode -format

#starting the cluster
start-dfs.sh
start-yarn.sh


jps #check java processes (view hadoop daemons)



hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/hduser
hdfs dfs -put hadoop-3.3.0.tar.gz /user/hduser

yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-*examples*.jar pi 5 10


WebUI
NN:9870

